---
title: "AdaSplit: Adaptive Split Learning"
collection: publications
permalink: /publication/adasplit
# excerpt: 'This paper is about the number 2. The number 3 is left for future work.'
date: 2022-01-01
venue: 'Preprint'
paperurl: 'https://arxiv.org/abs/2112.01637'
citation: 'Chopra, A., Sahu, S. K., Singh, A., <b>Java, A.</b>, Vepakomma, P., Sharma, V., & Raskar, R. (2021). AdaSplit: Adaptive Trade-offs for Resource-constrained Distributed Deep Learning. arXiv preprint arXiv:2112.01637.'
image: '../images/adasplit.png'
---
[Split learning](https://splitlearning.mit.edu/) (SL), a recent framework, reduces client compute load by splitting the model training between client and server. This flexibility is extremely useful for low-compute setups but is often achieved at cost of increase in bandwidth consumption and may result in sub-optimal convergence, especially when client data is heterogeneous. In this work, we introduce AdaSplit which enables efficiently scaling SL to low resource scenarios by reducing bandwidth consumption and improving performance across heterogeneous clients. (<i>Under Review</i>)<br>
This work was done in collaboration with the [MIT Media Lab](https://www.media.mit.edu/)
